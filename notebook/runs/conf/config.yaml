defaults:
  - _self_

model: HuggingFaceTB/SmolLM2-1.7B-Instruct
lr: 4e-3
num_epochs: 50
per_device_train_batch_size: 10
logging_steps: 2
device: cuda
prompt_tuning: false
token_tuning: false
reft: true

# Added Qwen-7B specific configurations
num_virtual_tokens: 3
token_dim: 4096
num_attention_heads: 32
num_layers: 32
model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
prompt_tuning_init_text: "Json output"
config_id: "smolLM2"
batch_size: 9
accumulation_steps: 4