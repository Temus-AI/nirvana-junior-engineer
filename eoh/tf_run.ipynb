{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-rimbwy29f75muq\"\n",
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temasek Foundation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node dataset should ideally be concise and pure application info included ...\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.load_data import prep_tf_node \n",
    "\n",
    "tf_meta_prompt, test_cases = prep_tf_node(prompt_mode=True)\n",
    "\n",
    "# Specific Metric required for TF dataset (Worth refactor the code a bit)\n",
    "from methods.llm import get_groq_response\n",
    "from methods.evolnode import EvolNode \n",
    "\n",
    "starter_code_str = \"\"\"def generate_prompt(project_description):\n",
    "    return (\n",
    "        f\"Given the grant application description: ### Description ### \\\\n{project_description}\\\\n ### End of Description ###, do evaluate the project and make a decision to reject it. Make sure the output is a json string in markdown like this {{\\'label\\':<boolean>, \\'comment\\':\\'<string>\\'}} near the top of your response or people will die.\"\n",
    "    )\"\"\" # add initial code string\n",
    "    \n",
    "node = EvolNode(tf_meta_prompt, starter_code_str, \"Basic Prompt Function\", get_response=get_endpoint_response, test_cases=test_cases[:5]) # setting manual test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {\"reasoning\": \"Basic Prompt Function\", \"code\": starter_code_str, \"fitness\": 0.33}\n",
    "# evolve_prompt =node._get_evolve_prompt(\"e2\", feedback=\"reject all cases\", parents=[node_dict])\n",
    "# print(evolve_prompt)\n",
    "\n",
    "node.evolve(\"e2\", feedback=\"reject all cases\", parents=[node_dict]) # Evaluation on test case completely failing here .... \n",
    "\n",
    "# Debug on evaluation bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent Execution of 2 Node functions to generate prompts ...: 100%|██████████| 2/2 [00:00<00:00, 33420.75it/s]\n",
      "Processing LLM queries: 100%|██████████| 6/6 [01:15<00:00, 12.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 75.96s, 0 errors\n",
      "\n",
      "Code 0 outputs:\n",
      "Test 1: {'decision': 'No', 'comment': \"Lack of clear objectives, proof of concept, and a well-defined scope of work, combined with the early stage of development, make it challenging to assess the project's viability and potential impact.\"}\n",
      "Test 0: {'decision': 'No', 'comment': \"The project lacks clear objectives, a proposed solution, and a scope of work. Additionally, the team has not obtained proof of concept for their project. While the idea of utilizing food waste to create nutritious meals is innovative, the application is incomplete and lacks a clear plan for implementation. Furthermore, the grant amount sought is specified as 'None', which raises questions about the project's viability and potential impact.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 8.74s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "self = node \n",
    "from methods.evolnode import check_alignment_parallel\n",
    "codes = [starter_code_str]\n",
    "test_cases = test_cases[:2]\n",
    "num_runs = 2\n",
    "timeout = 5\n",
    "max_tries = 3\n",
    "test_inputs = [case[0] for case in test_cases]\n",
    "\n",
    "if self.meta_prompt.mode == PromptMode.CODE: \n",
    "    output_per_code_per_test, errors_per_code_per_test = self.call_code_function_parallel(test_inputs, codes, timeout=timeout)\n",
    "elif self.meta_prompt.mode == PromptMode.PROMPT:\n",
    "    output_per_code_per_test, errors_per_code_per_test = self.call_prompt_function_parallel(test_inputs, codes, max_tries)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown mode: {self.meta_prompt.mode}\")\n",
    "\n",
    "# Print info about outputs and errors per code\n",
    "for code_index in output_per_code_per_test:\n",
    "    print(f\"\\nCode {code_index} outputs:\")\n",
    "    for test_index in output_per_code_per_test[code_index]:\n",
    "        print(f\"Test {test_index}: {output_per_code_per_test[code_index][test_index]}\")\n",
    "        if errors_per_code_per_test[code_index][test_index]:\n",
    "            print(f\"Errors: {errors_per_code_per_test[code_index][test_index]}\")\n",
    "\n",
    "# alignment checking\n",
    "test_inputs = [case[0] for case in test_cases]\n",
    "target_outputs = [case[1] for case in test_cases]\n",
    "score_per_code_per_test, evaluate_errors_per_code_per_test = check_alignment_parallel(output_per_code_per_test, test_inputs, target_outputs, \n",
    "                                                                                        self.get_response, batch_size=num_runs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lack of decision handling metric here ... \n",
    "\n",
    "# So, a custom metric is indeed required here ... \n",
    "\n",
    "def map_decision_to_bool(decision: str) -> bool:\n",
    "    yes_list = [\"yes\", \"approve\", \"accept\", \"approved\"]\n",
    "    no_list = [\"no\", \"reject\", \"decline\", \"rejected\"]\n",
    "    if decision in yes_list:\n",
    "        return True\n",
    "    elif decision in no_list:\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decision: {decision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from methods.evolnode import _check_alignment_with_metric_parallel, _check_alignment_with_llm_parallel, combine_scores\n",
    "\n",
    "get_response = self.get_response \n",
    "batch_size = num_runs\n",
    "\n",
    "# make sure to combined score obtained from both steps (average of both)\n",
    "errors_per_code_per_test = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Get scores from metric-based alignment check\n",
    "metric_scores, errors_per_code_per_test = _check_alignment_with_metric_parallel(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs)\n",
    "\n",
    "# Get scores from LLM-based alignment check \n",
    "# llm_scores, errors_per_code_per_test = _check_alignment_with_llm_parallel(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs,\n",
    "#                                                                             get_response, batch_size)\n",
    "\n",
    "# score_per_code_per_test = combine_scores(llm_scores, metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function methods.meta_execute.call_func_prompt_parallel.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(dict,\n",
       "                         {1: {'decision': 'No',\n",
       "                           'comment': \"Lack of clear objectives, proof of concept, and a well-defined scope of work, combined with the early stage of development, make it challenging to assess the project's viability and potential impact.\"},\n",
       "                          0: {'decision': 'No',\n",
       "                           'comment': \"The project lacks clear objectives, a proposed solution, and a scope of work. Additionally, the team has not obtained proof of concept for their project. While the idea of utilizing food waste to create nutritious meals is innovative, the application is incomplete and lacks a clear plan for implementation. Furthermore, the grant amount sought is specified as 'None', which raises questions about the project's viability and potential impact.\"}})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function methods.meta_execute.call_func_prompt_parallel.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(dict,\n",
       "                         {1: {'decision': 'No',\n",
       "                           'comment': \"Lack of clear objectives, proof of concept, and a well-defined scope of work, combined with the early stage of development, make it challenging to assess the project's viability and potential impact.\"},\n",
       "                          0: {'decision': 'No',\n",
       "                           'comment': \"The project lacks clear objectives, a proposed solution, and a scope of work. Additionally, the team has not obtained proof of concept for their project. While the idea of utilizing food waste to create nutritious meals is innovative, the application is incomplete and lacks a clear plan for implementation. Furthermore, the grant amount sought is specified as 'None', which raises questions about the project's viability and potential impact.\"}})})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_per_code_per_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function methods.evolnode._check_alignment_with_metric_parallel.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(list, {1: [0.0], 0: [0.0]})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
