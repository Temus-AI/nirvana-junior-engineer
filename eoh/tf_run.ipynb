{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-rimbwy29f75muq\"\n",
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temasek Foundation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node dataset should ideally be concise and pure application info included ...\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.load_data import prep_tf_node, custom_metric_map\n",
    "\n",
    "tf_meta_prompt, test_cases = prep_tf_node(prompt_mode=True)\n",
    "\n",
    "# Specific Metric required for TF dataset (Worth refactor the code a bit)\n",
    "from methods.llm import get_groq_response\n",
    "from methods.evolnode import EvolNode \n",
    "\n",
    "starter_code_str = \"\"\"def generate_prompt(project_description):\n",
    "    return (\n",
    "        f\"Given the grant application description: ### Description ### \\\\n{project_description}\\\\n ### End of Description ###, do evaluate the project and make a decision to reject it. Make sure the output is a json string in markdown like this {{\\'decision\\':<boolean>, \\'comment\\':\\'<string>\\'}} near the top of your response or people will die.\"\n",
    "    )\"\"\" # add initial code string\n",
    "    \n",
    "node = EvolNode(tf_meta_prompt, starter_code_str, \"Basic Prompt Function\", get_response=get_endpoint_response, test_cases=test_cases[:5]) # setting manual test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {\"reasoning\": \"Basic Prompt Function\", \"code\": starter_code_str, \"fitness\": 0.33}\n",
    "# evolve_prompt =node._get_evolve_prompt(\"e2\", feedback=\"reject all cases\", parents=[node_dict])\n",
    "# print(evolve_prompt)\n",
    "\n",
    "node.evolve(\"e2\", feedback=\"reject all cases\", parents=[node_dict]) # Evaluation on test case completely failing here .... \n",
    "\n",
    "# Debug on evaluation bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent Execution of 2 Node functions to generate prompts ...: 100%|██████████| 2/2 [00:00<00:00, 25343.23it/s]\n",
      "Processing LLM queries: 100%|██████████| 6/6 [00:10<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 10.38s, 0 errors\n",
      "\n",
      "Code 0 outputs:\n",
      "Test 0: {'decision': 'No', 'comment': \"The project has several critical issues, including a lack of clear objectives, proof of concept, and proposed solution. Furthermore, the grant amount sought is listed as none, which raises questions about the project's feasibility and scalability. Additionally, the project's scope of work is undefined, making it difficult to assess its viability and potential impact.\"}\n",
      "Test 1: {'decision': 'No', 'comment': \"The project, 'Marine Battery As A Service,' lacks a clear proof of concept and is in the ideation stage, which increases the risk of project failure. Additionally, the objectives and scope of work are not defined, making it challenging to assess the project's viability and potential impact. Furthermore, the absence of a specific grant amount sought suggests that the project may not be well-prepared for funding. While the project has engaged with key stakeholders, the lack of a clear plan and undefined objectives makes it difficult to justify funding at this stage.\"}\n",
      "Errors: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \\nunterminated string literal (detected at line 1) (<unknown>, line 1)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 4/4 [00:16<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 16.72s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "self = node \n",
    "from methods.evolnode import check_alignment_parallel\n",
    "\n",
    "codes = [starter_code_str]\n",
    "test_cases = test_cases[:2]\n",
    "num_runs = 2\n",
    "timeout = 5\n",
    "max_tries = 3\n",
    "test_inputs = [case[0] for case in test_cases]\n",
    "\n",
    "if self.meta_prompt.mode == PromptMode.CODE: \n",
    "    output_per_code_per_test, errors_per_code_per_test = self.call_code_function_parallel(test_inputs, codes, timeout=timeout)\n",
    "elif self.meta_prompt.mode == PromptMode.PROMPT:\n",
    "    output_per_code_per_test, errors_per_code_per_test = self.call_prompt_function_parallel(test_inputs, codes, max_tries)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown mode: {self.meta_prompt.mode}\")\n",
    "\n",
    "# Print info about outputs and errors per code\n",
    "for code_index in output_per_code_per_test:\n",
    "    print(f\"\\nCode {code_index} outputs:\")\n",
    "    for test_index in output_per_code_per_test[code_index]:\n",
    "        print(f\"Test {test_index}: {output_per_code_per_test[code_index][test_index]}\")\n",
    "        if errors_per_code_per_test[code_index][test_index]:\n",
    "            print(f\"Errors: {errors_per_code_per_test[code_index][test_index]}\")\n",
    "\n",
    "# alignment checking\n",
    "test_inputs = [case[0] for case in test_cases]\n",
    "target_outputs = [case[1] for case in test_cases]\n",
    "score_per_code_per_test, evaluate_errors_per_code_per_test = check_alignment_parallel(output_per_code_per_test, test_inputs, target_outputs, \n",
    "                                                                                        self.get_response, batch_size=num_runs, custom_metric_map=custom_metric_map) # not sure if buggy or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function methods.meta_execute.combine_scores.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(float, {0: 1.0, 1: 1.0})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_per_code_per_test\n",
    "score_per_code_per_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
